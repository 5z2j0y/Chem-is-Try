# Chem-is-Try

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8+-green.svg)
![Framework](https://img.shields.io/badge/Framework-MediaPipe_|_YOLO-orange.svg)

> Intelligent Chemical Experiment Operation Detection System Based on Computer Vision

## ğŸ“– Project Introduction

Chem-is-Try is an intelligent chemical experiment assistance system based on Orange Pi (RK3588), implementing real-time monitoring and recognition of experimental operations through computer vision technology. The system can accurately identify laboratory equipment and detect operational actions, providing intelligent assistance for chemical experiments.

### âœ¨ Core Features

- ğŸ” Laboratory Equipment Instance Segmentation (YOLO11-seg)
- ğŸ‘‹ Precise Hand Action Recognition (MediaPipe)
- âš¡ Multi-threaded Parallel Inference Architecture
- ğŸ¯ Support for Various Operation Detection:
  - Equipment Grasping Recognition
  - Liquid Pouring Detection
  - Titration Operation Monitoring
  - More operations under development...

## ğŸš€ Quick Start

### Requirements

- Orange Pi or other RK3588 development board
- Python 3.8+
- USB Camera

### Installation

1. Clone the project
```bash
git clone https://github.com/yourusername/Chem-is-Try.git
cd Chem-is-Try
```

2. Install dependencies
```bash
pip install -r requirements.txt
```

### Usage Examples

```bash
# Run gesture recognition demo
python static_gesture.py

# Run grasp detection demo
python grab_something.py

# Run instance segmentation demo
python Instance_Segmentation/yolo/yolo_seg.py
```

## ğŸ“ Project Structure

```
Chem-is-Try/
â”œâ”€â”€ datasets/              # Dataset resources
â”œâ”€â”€ models/               # Pre-trained models
â”œâ”€â”€ playground/           # Work-in-progress code
â”œâ”€â”€ process_data/         # Data processing tools
â”œâ”€â”€ utils/               # Utility functions
â”œâ”€â”€ voices/              # Voice prompt resources
â”œâ”€â”€ refrences.old        # Old references(Not used)
â”œâ”€â”€ requirements.txt     # Project dependencies
â””â”€â”€ README.md           # Project documentation

```

## ğŸ› ï¸ Technical Implementation

### Equipment Detection
- Uses YOLO11-seg for precise instance segmentation
- Supports simultaneous recognition of multiple equipment types
- Real-time tracking of equipment position and status

### Gesture Recognition
- Extracts 21 hand landmarks using MediaPipe
- Real-time hand gesture state analysis
- Precise action trajectory tracking

### Performance Optimization
- Uses tflite models for accelerated inference

## ğŸ’¡ Custom Training

### Data Preparation
```bash
# Convert dataset format
python process_data/coco2yolo.py
```

### Model Training
```bash
# Start training
python Object_Detection/yolo/yolo_train.py
```

## ğŸ¤ Contributing

1. Fork this project
2. Create a new feature branch
3. Submit your changes
4. Create a Pull Request

## ğŸ“„ License

This project is licensed under the [MIT](LICENSE) License.

## ğŸ™ Acknowledgments

- [YOLO](https://github.com/ultralytics/yolov5)
- [MediaPipe](https://mediapipe.dev/)

## ğŸ“® Contact

- Project Lead: [Your Name]
- Email: your.email@example.com
- Issues: [GitHub Issues](https://github.com/yourusername/Chem-is-Try/issues)